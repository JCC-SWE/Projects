{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HM1: Logistic Regression.\n",
    "\n",
    "### Name: Joshua Cubero\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also, you should plot their objective values versus epochs and compare their training and testing accuracy. You will need to tune the parameters a little bit to obtain reasonable results.\n",
    "\n",
    "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Packages\n",
    "import numpy as np\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data processing\n",
    "\n",
    "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
    "- Load the data.\n",
    "- Preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  texture_worst  perimeter_worst  area_worst  smoothness_worst  \\\n",
       "0  ...          17.33           184.60      2019.0            0.1622   \n",
       "1  ...          23.41           158.80      1956.0            0.1238   \n",
       "2  ...          25.53           152.50      1709.0            0.1444   \n",
       "3  ...          26.50            98.87       567.7            0.2098   \n",
       "4  ...          16.67           152.20      1575.0            0.1374   \n",
       "\n",
       "   compactness_worst  concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "0             0.6656           0.7119                0.2654          0.4601   \n",
       "1             0.1866           0.2416                0.1860          0.2750   \n",
       "2             0.4245           0.4504                0.2430          0.3613   \n",
       "3             0.8663           0.6869                0.2575          0.6638   \n",
       "4             0.2050           0.4000                0.1625          0.2364   \n",
       "\n",
       "   fractal_dimension_worst  Unnamed: 32  \n",
       "0                  0.11890          NaN  \n",
       "1                  0.08902          NaN  \n",
       "2                  0.08758          NaN  \n",
       "3                  0.17300          NaN  \n",
       "4                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Examine and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                           0\n",
       "diagnosis                    0\n",
       "radius_mean                  0\n",
       "texture_mean                 0\n",
       "perimeter_mean               0\n",
       "area_mean                    0\n",
       "smoothness_mean              0\n",
       "compactness_mean             0\n",
       "concavity_mean               0\n",
       "concave points_mean          0\n",
       "symmetry_mean                0\n",
       "fractal_dimension_mean       0\n",
       "radius_se                    0\n",
       "texture_se                   0\n",
       "perimeter_se                 0\n",
       "area_se                      0\n",
       "smoothness_se                0\n",
       "compactness_se               0\n",
       "concavity_se                 0\n",
       "concave points_se            0\n",
       "symmetry_se                  0\n",
       "fractal_dimension_se         0\n",
       "radius_worst                 0\n",
       "texture_worst                0\n",
       "perimeter_worst              0\n",
       "area_worst                   0\n",
       "smoothness_worst             0\n",
       "compactness_worst            0\n",
       "concavity_worst              0\n",
       "concave points_worst         0\n",
       "symmetry_worst               0\n",
       "fractal_dimension_worst      0\n",
       "Unnamed: 32                569\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
    "# You need to get rid of the ID number feature.\n",
    "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
    "df = df.replace({'M': 1,'B': -1})\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use of iloc and slicing leaves out ID column.\n",
    "X = np.array([df.iloc[:,2:32]]).reshape(569,30)\n",
    "y = np.array(df['diagnosis']).reshape(569,1)\n",
    "d = X.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Partition to training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machine learning.\n",
    "split = int(0.8*len(X))\n",
    "x_train, x_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the standardization to transform both training and test features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test mean = \n",
      "[-0.15421396  0.36734644 -0.15216679 -0.13011779 -0.03764451 -0.06095979\n",
      " -0.15968807 -0.18264447 -0.20577445  0.11252658 -0.11462619  0.14746728\n",
      " -0.09472252 -0.06878809  0.08026941 -0.09071034 -0.08845148 -0.1073992\n",
      " -0.22935047 -0.02350692 -0.18689467  0.21990882 -0.1751222  -0.16131338\n",
      " -0.03903494 -0.1155415  -0.14820649 -0.19491169 -0.31065331 -0.01879552]\n",
      "test std = \n",
      "[1.02409598 1.20104314 1.02823038 1.05254889 1.12623735 0.8975497\n",
      " 0.92492381 0.93975209 0.84870282 1.0407246  1.11662724 1.16849856\n",
      " 1.10827607 1.35656254 1.01798397 0.78409753 0.6656818  0.94213695\n",
      " 0.67826598 0.75174437 0.98497496 1.06139417 1.00696616 1.02714708\n",
      " 0.95963507 0.82460324 0.90787878 0.89791033 0.72641788 0.89866581]\n"
     ]
    }
   ],
   "source": [
    "# Standardization\n",
    "import numpy\n",
    "\n",
    "# calculate mu and sig using the training set\n",
    "d = x_train.shape[1]\n",
    "mu = numpy.mean(x_train, axis=0).reshape(1, d)\n",
    "sig = numpy.std(x_train, axis=0).reshape(1, d)\n",
    "\n",
    "# transform the training features\n",
    "x_train = (x_train - mu) / (sig + 1E-6)\n",
    "\n",
    "# transform the test features\n",
    "x_test = (x_test - mu) / (sig + 1E-6)\n",
    "\n",
    "print('test mean = ')\n",
    "print(numpy.mean(x_test, axis=0))\n",
    "\n",
    "print('test std = ')\n",
    "print(numpy.std(x_test, axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30)\n",
      "(114, 30)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.  Logistic Regression Model\n",
    "\n",
    "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
    "\n",
    "When $\\lambda = 0$, the model is a regular logistic regression and when $\\lambda > 0$, it essentially becomes a regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective function value, or loss\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     objective function value, or loss (scalar)\n",
    "def objective(w, x, y, lam):\n",
    "    wTx = np.dot(x,w)\n",
    "    z = np.multiply(y,wTx)\n",
    "    exp_z = 1 + np.exp(-z)\n",
    "    l_z = np.log(exp_z)\n",
    "    l2_reg = l_z + ((lam/2) * np.sum(w*w))\n",
    "    obj = np.mean(l2_reg)\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Gradient descent\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the gradient\n",
    "# Inputs:\n",
    "#     w: weight: d-by-1 matrix\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: regularization parameter: scalar\n",
    "# Return:\n",
    "#     g: gradient: d-by-1 matrix\n",
    "\n",
    "def gradient(w,x,y,lam):\n",
    "    n,d = x.shape\n",
    "    yx = np.multiply(y,x)\n",
    "    wTx = np.dot(x,w)\n",
    "    z = np.multiply(y,wTx)\n",
    "    denominator = 1 + np.exp(z)\n",
    "    quotient = (-yx)/denominator     \n",
    "    g = np.mean(quotient,axis=0).reshape(d,1) + (lam*w) \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent for solving logistic regression\n",
    "# You will need to do iterative processes (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "\n",
    "def gradient_descent(x, y, lam, learning_rate, w = None, max_epoch=100):\n",
    "    if w == None:\n",
    "        w = np.zeros((x.shape[1],1))\n",
    "    obj_vals = np.zeros(max_epoch)\n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "        obj = objective(w,x,y,lam)\n",
    "        obj_vals[i] = obj\n",
    "        gd = gradient(w,x,y,lam)\n",
    "        w -= learning_rate * gd\n",
    "    return w, obj_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "wbgd, obj_1 = gradient_descent(x_train,y_train,0,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "l2_wbgd, obj_2 = gradient_descent(x_train,y_train,0.1,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Stochastic gradient descent (SGD)\n",
    "\n",
    "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
    "\n",
    "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_i and the gradient of Q_i\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     xi: data: 1-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def stochastic_objective_gradient(w, xi, yi, lam):\n",
    "    #Stochastic Objective\n",
    "    d = xi.shape[0]\n",
    "    xTw = np.dot(xi.reshape(1,30),w)    \n",
    "    z = yi * xTw\n",
    "    log_z = np.log(1 + np.exp(-z))    \n",
    "    obj = log_z + (lam/2) * np.sum(w * w)\n",
    "    \n",
    "    #Stochastic Gradient\n",
    "    yi_xi = np.multiply(yi,xi).reshape(1,30)\n",
    "    cost = -yi_xi.T / (1 + np.exp(z))\n",
    "    gdt = cost + (lam * w)\n",
    "    return obj,gdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples.\n",
    "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     \n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "def sgd(x, y, lam, learning_rate, w = None, max_epoch=100):\n",
    "    n,d = x.shape\n",
    "    sgd_obj_vals = np.zeros(max_epoch)\n",
    "    \n",
    "    if w is None:\n",
    "        w = np.zeros((d,1))\n",
    "        \n",
    "    for i in range(max_epoch):\n",
    "        rand_i = np.random.permutation(n)\n",
    "        random_x = x[rand_i, :]\n",
    "        random_y = y[rand_i, :]\n",
    "        sgd_obj_val = 0\n",
    "        \n",
    "        for j in range(n):\n",
    "            xi = random_x[j, :].reshape(1,30)\n",
    "            yi = float(random_y[j, :])            \n",
    "            obj, gdt = stochastic_objective_gradient(w, xi, yi, lam = 0)\n",
    "            sgd_obj_val += obj\n",
    "            w -= learning_rate * gdt\n",
    "        learning_rate *= .9\n",
    "        sgd_obj_val /= n\n",
    "        sgd_obj_vals[i] = sgd_obj_val\n",
    "    return w, sgd_obj_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "wsgd, obj3 = sgd(x_train,y_train,lam=0,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "l2_wsgd, obj4 = sgd(x_train,y_train,lam=0.1,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Mini-Batch Gradient Descent (MBGD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
    "\n",
    "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
    "\n",
    "You may need to implement a new function to calculate the new objective function and gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the objective Q_I and the gradient of Q_I\n",
    "# Inputs:\n",
    "#     w: weights: d-by-b matrix\n",
    "#     xi: data: b-by-d matrix\n",
    "#     yi: label: scalar\n",
    "#     lam: scalar, the regularization parameter\n",
    "# Return:\n",
    "#     obj: scalar, the objective Q_i\n",
    "#     g: d-by-1 matrix, gradient of Q_i\n",
    "\n",
    "def mb_objective_gradient(w, xi, yi, lam = 0):\n",
    "    n,d = xi.shape\n",
    "    #\n",
    "    wTx = np.dot(xi,w)\n",
    "    z = np.multiply(yi,wTx)\n",
    "    exp_z = 1 + np.exp(-z)\n",
    "    l_z = np.log(exp_z)\n",
    "    l2_reg = l_z + ((lam/2) * np.sum(w*w))\n",
    "    obj = np.mean(l2_reg)\n",
    "    \n",
    "    yx = np.multiply(yi,xi)\n",
    "    wTx = np.dot(xi,w)\n",
    "    z = np.multiply(yi,wTx)\n",
    "    denominator = 1 + np.exp(z)\n",
    "    quotient = (-yx)/denominator     \n",
    "    g = np.mean(quotient,axis=0).reshape(d,1) + (lam*w) \n",
    "    return obj, g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hints:\n",
    "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
    "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MBGD for solving logistic regression\n",
    "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
    "\n",
    "# Inputs:\n",
    "#     x: data: n-by-d matrix\n",
    "#     y: label: n-by-1 matrix\n",
    "#     lam: scalar, the regularization parameter\n",
    "#     learning_rate: scalar\n",
    "#     w: weights: d-by-1 matrix, initialization of w\n",
    "#     max_epoch: integer, the maximal epochs\n",
    "# Return:\n",
    "#     w: weights: d-by-1 matrix, the solution\n",
    "#     objvals: a record of each epoch's objective value\n",
    "#     Record one objective value per epoch (not per iteration)\n",
    "\n",
    "\n",
    "def mbgd(x, y, b, lam = 0, learning_rate = 1.0, w = None, max_epoch=100):\n",
    "    n,d = x.shape\n",
    "    mbgd_obj_vals = np.zeros(max_epoch)\n",
    "    iters = n//b\n",
    "    if w is None:\n",
    "        w = np.zeros((d,1))\n",
    "    \n",
    "    for i in range(max_epoch):\n",
    "        #Creates uniform sampled indicies.\n",
    "        rand_i = np.random.permutation(n)\n",
    "        random_x = x[rand_i, :]\n",
    "        random_y = y[rand_i, :]        \n",
    "        #indices = np.random.choice(n, b,replace = False)            \n",
    "        sgd_obj_val = 0\n",
    "        for j in range(iters):\n",
    "            #Calculates gradient and objective values\n",
    "            indices = np.random.choice(n, b,replace = False) \n",
    "            xi = random_x[indices,:]\n",
    "            yi = random_y[indices]\n",
    "            obj,gdt = mb_objective_gradient(w, xi, yi,lam)            \n",
    "            sgd_obj_val += obj             \n",
    "            w -= learning_rate * gdt\n",
    "        learning_rate *= .9\n",
    "        sgd_obj_val /= b\n",
    "        mbgd_obj_vals[i] = sgd_obj_val\n",
    "    return w, mbgd_obj_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "wmbgd,obj5 = mbgd(x_train, y_train, b = 50, lam = 0, learning_rate = 0.5, w = None, max_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regularized logistic regression\n",
    "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
    "l2_wmbgd,obj6 = mbgd(x_train, y_train, b = 50, lam = .1, learning_rate = 0.5, w = None, max_epoch=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare GD, SGD, MBGD\n",
    "\n",
    "### Plot objective function values against epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "iters = np.arange(0,100)\n",
    "plt.plot(iters,obj_1,label='GD')\n",
    "plt.plot(iters,obj3,color='red',label='SGD')\n",
    "plt.plot(iters,obj5,label='MBGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Objective Values')\n",
    "plt.title('Non-Regularized Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot depicts the results of the three non-regularized logistic regression models using BGD, SGD, and MBGD. Upon visual analysis one can see that GD requires the greatest number of epochs to find the minimum, whereas SGD and MBGD appear to find their minimums much more efficiently. However, because MBGD uses random variables each epoch, it's plot continues to display variation through all epochs. Lastly, MBGD appears to find the closest minimum to zero out of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(iters,obj_2,label='GD')\n",
    "plt.plot(iters,obj4,color='red',label='SGD')\n",
    "plt.plot(iters,obj6,label='MBGD')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Objective Values')\n",
    "plt.title('Regularized Logistic Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regularized logistic regression model includes the l-2 norm as a penalty to prevent overfitting. The SGD and MBGD regularized models appear to return similar results to the non-regularized logistic regression models. However, BGD appear to underperform the non-regularized models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Prediction\n",
    "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict class label\n",
    "# Inputs:\n",
    "#     w: weights: d-by-1 matrix\n",
    "#     X: data: m-by-d matrix\n",
    "# Return:\n",
    "#     f: m-by-1 matrix, the predictions\n",
    "def predict(w, X):\n",
    "    n, d = X.shape\n",
    "    z = np.dot(X,w)\n",
    "    f_x = np.log(1 + np.exp(-z))\n",
    "    predictions = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        if f_x[i] > 1:\n",
    "            predictions[i] = -1\n",
    "        elif f_x[i] <= 1:\n",
    "            predictions[i] = 1\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate(y_true,y_pred):\n",
    "    n = y_test.shape[0]\n",
    "    errors = 0\n",
    "    for i in range(n):\n",
    "        if y_pred[i] != y_true[i]:\n",
    "            errors += 1\n",
    "    rate = errors/n\n",
    "    return rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictions for test tr\n",
    "y_hat_mbgd_test = predict(wmbgd,x_test)\n",
    "y_hat_sgd_test = predict(wsgd,x_test)\n",
    "y_hat_wbgd_test = predict(wbgd,x_test)\n",
    "y_hat_l2_bdg_test = predict(l2_wbgd,x_test)\n",
    "y_hat_l2_sdg_test = predict(l2_wsgd,x_test)\n",
    "y_hat_l2_mbdg_test = predict(l2_wmbgd,x_test)\n",
    "\n",
    "y_hat_mbgd = predict(wmbgd,x_train)\n",
    "y_hat_sgd = predict(wsgd,x_train)\n",
    "y_hat_wbgd = predict(wbgd,x_train)\n",
    "y_hat_l2_bdg = predict(l2_wbgd,x_train)\n",
    "y_hat_l2_sdg = predict(l2_wsgd,x_train)\n",
    "y_hat_l2_mbdg = predict(l2_wmbgd,x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error Gradient Descent: 0.043859649122807015\n",
      "Training Error Stochastic Gradient Descent: 0.017543859649122806\n",
      "Training Error Mini-Batch Gradient Descent: 0.02631578947368421\n",
      "-------------------------------------------------------\n",
      "Training Error L-2 Gradient Descent: 0.03508771929824561\n",
      "Training Error L-2 Stochastic Gradient Descent: 0.017543859649122806\n",
      "Training Error L-2 Mini-Batch Gradient Descent: 0.03508771929824561\n"
     ]
    }
   ],
   "source": [
    "# evaluate training error of logistic regression and regularized version\n",
    "error1 = error_rate(y_train,y_hat_wbgd)\n",
    "error2 = error_rate(y_train, y_hat_sgd)\n",
    "error3 = error_rate(y_train, y_hat_mbgd)\n",
    "error4 = error_rate(y_train, y_hat_l2_bdg)\n",
    "error5 = error_rate(y_train, y_hat_l2_sdg)\n",
    "error6 = error_rate(y_train, y_hat_l2_mbdg)\n",
    "\n",
    "print(f'Training Error Gradient Descent: {error1}')\n",
    "print(f'Training Error Stochastic Gradient Descent: {error2}')\n",
    "print(f'Training Error Mini-Batch Gradient Descent: {error3 }')\n",
    "print('-------------------------------------------------------')\n",
    "print(f'Training Error L-2 Gradient Descent: {error4}')\n",
    "print(f'Training Error L-2 Stochastic Gradient Descent: {error5}')\n",
    "print(f'Training Error L-2 Mini-Batch Gradient Descent: {error6 }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon analysis of the training sets for both L-2 and non-regularized logistic regression returned similar results, plus or minus 0.5 - 1. Overall, SGD performed best, and BGD performed worst of the three models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error Gradient Descent: 0.07017543859649122\n",
      "Testing Error Stochastic Gradient Descent: 0.043859649122807015\n",
      "Testing Error Mini-Batch Gradient Descent: 0.06140350877192982\n",
      "------------------------------------------------------------\n",
      "Testing Error L-2 Gradient Descent: 0.08771929824561403\n",
      "Testing Error L-2 Stochastic Gradient Descent: 0.043859649122807015\n",
      "Testing Error L-2 Mini-Batch Gradient Descent: 0.08771929824561403\n"
     ]
    }
   ],
   "source": [
    "# evaluate testing error of logistic regression and regularized version\n",
    "error7 = error_rate(y_test,y_hat_wbgd_test)\n",
    "error8 = error_rate(y_test, y_hat_sgd_test)\n",
    "error9 = error_rate(y_test, y_hat_mbgd_test)\n",
    "error10 = error_rate(y_test, y_hat_l2_bdg_test)\n",
    "error11 = error_rate(y_test, y_hat_l2_sdg_test)\n",
    "error12 = error_rate(y_test, y_hat_l2_mbdg_test)\n",
    "\n",
    "print(f'Testing Error Gradient Descent: {error7}')\n",
    "print(f'Testing Error Stochastic Gradient Descent: {error8}')\n",
    "print(f'Testing Error Mini-Batch Gradient Descent: {error9}')\n",
    "print('------------------------------------------------------------')\n",
    "print(f'Testing Error L-2 Gradient Descent: {error10}')\n",
    "print(f'Testing Error L-2 Stochastic Gradient Descent: {error11}')\n",
    "print(f'Testing Error L-2 Mini-Batch Gradient Descent: {error12}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, all six models saw increased error rate against the test data set. It does not come as a surprise that the models performed worse against the test data however. The SGD models performed the best with the BGD models again exhiting the worst performance. Some of the results are very similar due to their lamdas and learning rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Parameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first attempt at parameter tuning uses L2-BGD. With tuning, the best model returned a 0.0614 error rate, which is less than the initial return of 0.08772."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning1, obj_tune_1 = gradient_descent(x_train,y_train,0.01,0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tuning1 = predict(tuning1,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.06140350877192982"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate(y_test,y_tuning1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performance for the L2-SGD model occurs at lambda = 0.01 and learning rate = 0.1. Any parameters less than these values \n",
    "is immaterial, and any inputs greater than these values increases the error rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning2, obj_tune2 = sgd(x_train,y_train,lam=0.01,learning_rate=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tuning2 = predict(tuning2,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043859649122807015"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate(y_test,y_tuning2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performance for MBGD on this dataset occurrs at batchsize = 50, lambda = 0.05, and learning rate = 0.1. An error rate of 0.079 is just less than the untuned model's error rate of 0.0877."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning3,obj_tune3 = mbgd(x_train, y_train, b = 50, lam = .05, learning_rate = 0.1, w = None, max_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tuning3 = predict(tuning3,x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08771929824561403"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_rate(y_test,y_tuning3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
